{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://cedric.cnam.fr/~ferecatu/RCP216/tp/tptexte/lsa.jar\n",
      "Finished download of lsa.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://cedric.cnam.fr/~ferecatu/RCP216/tp/tptexte/lsa.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg.{DenseMatrix=>BDenseMatrix, DenseVector=>BDenseVector, SparseVector=>BSparseVector, Vector=>BVector}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.rdd\n",
    "\n",
    "import scala.xml._\n",
    "\n",
    "import org.apache.hadoop.io.{ Text, LongWritable }\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import com.cloudera.datascience.common.XmlInputFormat\n",
    "import com.cloudera.datascience.lsa.ParseWikipedia._\n",
    "import com.cloudera.datascience.lsa.RunLSA._\n",
    "\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import breeze.linalg.{DenseMatrix => BDenseMatrix, DenseVector => BDenseVector, SparseVector => BSparseVector, Vector => BVector}\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "import java.io.StringReader\n",
    "import edu.stanford.nlp.ling.CoreLabel;\n",
    "import edu.stanford.nlp.ling.HasWord;\n",
    "import edu.stanford.nlp.process.CoreLabelTokenFactory;\n",
    "import edu.stanford.nlp.process.DocumentPreprocessor;\n",
    "import edu.stanford.nlp.process.PTBTokenizer;\n",
    "\n",
    "import java.sql.Timestamp\n",
    "import java.text.SimpleDateFormat\n",
    "\n",
    "import org.apache.spark.mllib.clustering.{ KMeans, KMeansModel }\n",
    "import org.apache.spark.mllib.util.KMeansDataGenerator\n",
    "\n",
    "import scala.collection.mutable.HashMap\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toBreeze: (v: org.apache.spark.mllib.linalg.Vector)breeze.linalg.Vector[Double]\n",
       "fromBreeze: (bv: breeze.linalg.Vector[Double])org.apache.spark.mllib.linalg.Vector\n",
       "add: (v1: org.apache.spark.mllib.linalg.Vector, v2: org.apache.spark.mllib.linalg.Vector)org.apache.spark.mllib.linalg.Vector\n",
       "scalarMultiply: (a: Double, v: org.apache.spark.mllib.linalg.Vector)org.apache.spark.mllib.linalg.Vector\n",
       "stackVectors: (v1: org.apache.spark.mllib.linalg.Vector, v2: org.apache.spark.mllib.linalg.Vector)Unit\n",
       "loadArticle: (sc: org.apache.spark.SparkContext, path: String)org.apache.spark.rdd.RDD[String]\n",
       "extractDate: (elem: scala.xml.Elem)java.sql.Timestamp\n",
       "extractString: (elem: scala.xml.Elem, field: String)String\n",
       "extractInt: (elem: scala.xml.Elem, field: String)Int\n",
       "extractAll: (elem: scala.xml.Elem, what...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def toBreeze(v:Vector) = BVector(v.toArray)\n",
    "def fromBreeze(bv:BVector[Double]) = Vectors.dense(bv.toArray)\n",
    "def add(v1:Vector, v2:Vector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
    "def scalarMultiply(a:Double, v:Vector) = fromBreeze(a * toBreeze(v))\n",
    "\n",
    "def stackVectors(v1:Vector, v2:Vector) = {\n",
    "var v3 = Vectors.zeros(v1.size+v2.size)\n",
    "    for (i <- 0 until v1.size) {\n",
    "      BVector(v3.toArray)(i) = v1(i);\n",
    "    }\n",
    "    for (i <- 0 until v2.size) {\n",
    "      BVector(v3.toArray)(v1.size+i) = v2(i);\n",
    "    }\n",
    "}\n",
    "\n",
    "// SÃ©paration du fichier XML en un RDD oÃ¹ chaque Ã©lÃ©ment est un article\n",
    "// Retourne un RDD de String Ã  partir du fichier \"path\"\n",
    "def loadArticle(sc: SparkContext, path: String): RDD[String] = {\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XmlInputFormat.START_TAG_KEY, \"<document>\")\n",
    "conf.set(XmlInputFormat.END_TAG_KEY, \"</document>\")\n",
    "val in = sc.newAPIHadoopFile(path, classOf[XmlInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
    "in.map(line => line._2.toString)\n",
    "}\n",
    "\n",
    "\n",
    "// Pour un élément XML de type \"document\",\n",
    "//   - on extrait le champ \"date\"\n",
    "//   - on parse la chaÃ®ne de caractÃ¨re au format yyyy-MM-dd HH:mm:ss\n",
    "//   - on retourne un Timestamp\n",
    "def extractDate(elem: scala.xml.Elem): java.sql.Timestamp = {\n",
    "    val dn: scala.xml.NodeSeq = elem \\\\ \"date\"\n",
    "    val x: String = dn.text\n",
    "    // d'aprÃ¨s l'exemple 2011-05-18 16:30:35\n",
    "    val format = new SimpleDateFormat(\"yyyy-MM-dd\")\n",
    "    if (x == \"\")\n",
    "      return null\n",
    "    else {\n",
    "      val d = format.parse(x.toString());\n",
    "      val t = new Timestamp(d.getTime());\n",
    "      return t\n",
    "    }\n",
    "}\n",
    "\n",
    "// Pour un élément XML de type \"document\",\n",
    "//   - on extrait le champ #field\n",
    "def extractString(elem: scala.xml.Elem, field: String): String = {\n",
    "    val dn: scala.xml.NodeSeq = elem \\\\ field\n",
    "    val x: String = dn.text\n",
    "    return x\n",
    "}\n",
    "\n",
    "def extractInt(elem: scala.xml.Elem, field: String): Int = {\n",
    "    val dn: scala.xml.NodeSeq = elem \\\\ field\n",
    "    val x: Int = dn.text.toInt\n",
    "    return x\n",
    "}\n",
    "\n",
    "def extractAll(elem: scala.xml.Elem, whatText: String = \"text\"): (Int, java.sql.Timestamp, String) = {\n",
    "    return (extractInt(elem,\"docid\"), extractDate(elem), extractString(elem,whatText))\n",
    "}\n",
    "\n",
    "def extractText(elem: scala.xml.Elem): String = {\n",
    "    return (extractString(elem,\"title\") + \" \" + extractString(elem,\"text\"))\n",
    "}\n",
    "\n",
    "// NÃ©cessaire, car le type java.sql.Timestamp n'est pas ordonnÃ© par dÃ©faut (étonnant...)\n",
    "implicit def ordered: Ordering[java.sql.Timestamp] = new Ordering[java.sql.Timestamp] {\n",
    "def compare(x: java.sql.Timestamp, y: java.sql.Timestamp): Int = x compareTo y\n",
    "}\n",
    "\n",
    "def hasLetters(str: String): Boolean = {\n",
    "// While loop for high performance\n",
    "var i = 0\n",
    "while (i < str.length) {\n",
    "  if (Character.isLetter(str.charAt(i))) {\n",
    "    return true\n",
    "  }\n",
    "  i += 1\n",
    "}\n",
    "false\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords = Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val stopwords = sc.textFile(\"stopwords.txt\").collect.toArray.toSet\n",
    "    val stopwordsBroadcast = sc.broadcast(stopwords).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w2vModel = org.apache.spark.mllib.feature.Word2VecModel@f6e9ec5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.Word2VecModel@f6e9ec5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.Word2VecModel\n",
    "// lire le Word2VecModel\n",
    "val w2vModel = Word2VecModel.load(sc, \"w2vModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\n",
       "org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
       "org.apache.toree.kernel.api.Kernel.sparkSession(Kernel.scala:444)\n",
       "org.apache.toree.kernel.api.Kernel.sparkContext(Kernel.scala:449)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
       "org.apache.toree.kernel.api.Kernel.addJars(Kernel.scala:80)\n",
       "org.apache.toree.magic.builtin.AddJar.execute(AddJar.scala:181)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.toree.plugins.PluginMethod$$anonfun$invoke$2.apply(PluginMethod.scala:116)\n",
       "scala.util.Try$.apply(Try.scala:192)\n",
       "org.apache.toree.plugins.PluginMethod.invoke(PluginMethod.scala:84)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:334)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:333)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "StackTrace: org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
       "org.apache.toree.kernel.api.Kernel.sparkSession(Kernel.scala:444)\n",
       "org.apache.toree.kernel.api.Kernel.sparkContext(Kernel.scala:449)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
       "org.apache.toree.kernel.api.Kernel.addJars(Kernel.scala:80)\n",
       "org.apache.toree.magic.builtin.AddJar.execute(AddJar.scala:181)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.toree.plugins.PluginMethod$$anonfun$invoke$2.apply(PluginMethod.scala:116)\n",
       "scala.util.Try$.apply(Try.scala:192)\n",
       "org.apache.toree.plugins.PluginMethod.invoke(PluginMethod.scala:84)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:334)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:333)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2483)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2479)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2479)\n",
       "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2568)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAppName(\"NEWS\").setMaster(\"local[*]\").set(\"spark.driver.memory\", \"4g\")\n",
    "\n",
    "\n",
    "\n",
    "conf.set(\"spark.hadoop.validateOutputSpecs\", \"false\")\n",
    "//conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "//conf.setMaster(\"local[*]\")\n",
    "//conf.set(\"spark.executor.memory\", MAX_MEMORY)\n",
    "//conf.set(\"spark.driver.memory\", MAX_MEMORY)\n",
    "//conf.set(\"spark.driver.maxResultSize\", MAX_MEMORY)\n",
    "val sc = new SparkContext(conf)\n",
    "\n",
    "\n",
    "val hadoopConf = new org.apache.hadoop.conf.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textToExtract = text\n",
       "useW2Vec = true\n",
       "weightTfIdf = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    var textToExtract = \"text\";\n",
    "    var useW2Vec = true;\n",
    "    var weightTfIdf = true;\n",
    "    println (\"*****\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nysk_raw = MapPartitionsRDD[126] at map at <console>:99\n",
       "nysk_xml = MapPartitionsRDD[127] at map at <console>:115\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[127] at map at <console>:115"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val news_raw = loadArticle(sc, \"articles1.xml\")/*.sample(false,0.01)*/\n",
    "    val news_xml: RDD[Elem] = news_raw.map(XML.loadString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "articles1 = [docid: string, title: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[docid: string, title: string ... 6 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val articles1 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\",\";\").load(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+--------------------+----------+----+-----+--------------------+\n",
      "|docid|               title|   publication|              author|      date|year|month|                text|\n",
      "+-----+--------------------+--------------+--------------------+----------+----+-----+--------------------+\n",
      "|17283|House Republicans...|New York Times|          Carl Hulse|31/12/2016|2016|   12|WASHINGTON  �   C...|\n",
      "|17291|First, a Mixtape....|New York Times|    Katherine Rosman|31/12/2016|2016|   12|Just how   is Hil...|\n",
      "|17292|Calling on Angels...|New York Times|         Andy Newman|31/12/2016|2016|   12|Angels are everyw...|\n",
      "|20113|Clinton, Sanders ...|New York Times|    James Poniewozik|16/04/2016|2016|    4|Thursday night, D...|\n",
      "|20114|Yahoo�s Suitors U...|New York Times|Vindu Goel and Mi...|18/04/2016|2016|    4|SAN FRANCISCO  � ...|\n",
      "+-----+--------------------+--------------+--------------------+----------+----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articles1.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|     publication|count|\n",
      "+----------------+-----+\n",
      "|Business Insider|   14|\n",
      "|             CNN| 4104|\n",
      "|  New York Times| 3628|\n",
      "|       Breitbart|16100|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "articlesfiltred = [publication: string]\n",
       "articlesfiltred1 = [publication: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val articlesfiltred=articles1.filter(\"month = 6\").select(\"publication\")\n",
    "val articlesfiltred1=articles1.filter(\"year = 2016\").groupBy(\"publication\").count()\n",
    "articlesfiltred1.show()\n",
    "articlesfiltred.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------+--------------------+----------+------+------------------+--------------------+\n",
      "|summary|             docid|               title|   publication|              author|      date|  year|             month|                text|\n",
      "+-------+------------------+--------------------+--------------+--------------------+----------+------+------------------+--------------------+\n",
      "|  count|             23846|               23846|         23846|               22219|     23846| 23846|             23846|               23846|\n",
      "|   mean| 41977.41763817831|                null|          null|                null|      null|2016.0| 7.213662668791412|                null|\n",
      "| stddev|10633.814549540011|                null|          null|                null|      null|   0.0|3.2824944658486532|                null|\n",
      "|    min|             17283|\"***LIVE At 5:30P...|     Breitbart|         A. O. Scott|01/01/2016|  2016|                 1|#AmericaWasNeverG...|\n",
      "|    max|             64839|��Santas� march a...|New York Times|�scar Mart�nez, E...|31/12/2016|  2016|                 9|��Que bol� Cuba?�...|\n",
      "+-------+------------------+--------------------+--------------+--------------------+----------+------+------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- docid: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- publication: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//calcul statistiques\n",
    "articles1.describe().show()\n",
    "articles1.count()\n",
    "schema = StructType([StructField(\"docid\", StringType(), True),StructField(\"source\", StringType(), True),StructField(\"url\", StringType(), True),StructField(\"title\", StringType(), True),StructField(\"summary\", StringType(), True),StructField(\"text\", StringType(), True),StructField(\"date\", StringType(), True)])\n",
    "\n",
    "\n",
    "articles1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nysk = MapPartitionsRDD[102] at map at <console>:134\n",
       "nyskTitles = MapPartitionsRDD[103] at map at <console>:135\n",
       "nyskSummaries = MapPartitionsRDD[104] at map at <console>:136\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[104] at map at <console>:136"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val news: RDD[(Int, java.sql.Timestamp, String)] = news_xml.map(e => extractAll(e,textToExtract))\n",
    "    val newsTitles: RDD[(Int, java.sql.Timestamp, String)] = news_xml.map(e => extractAll(e,\"title\"))\n",
    "    val newsSummaries: RDD[(Int, java.sql.Timestamp, String)] = news_xml.map(e => extractAll(e,\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords = Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val stopwords = sc.textFile(\"stopwords.txt\").collect.toArray.toSet\n",
    "    val stopwordsBroadcast = sc.broadcast(stopwords).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemmatizedWithDate = MapPartitionsRDD[107] at mapPartitions at <console>:133\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[107] at mapPartitions at <console>:133"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   val lemmatizedWithDate = news.mapPartitions(iter => {\n",
    "      val pipeline = com.cloudera.datascience.lsa.ParseWikipedia.createNLPPipeline();\n",
    "      iter.map {\n",
    "        case (docid, date, text) =>\n",
    "          (docid.toString, date,\n",
    "            com.cloudera.datascience.lsa.ParseWikipedia.plainTextToLemmas(text.toLowerCase.split(\"\\\\W+\").mkString(\" \"), stopwordsBroadcast, pipeline))\n",
    "        };\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemmatized = MapPartitionsRDD[108] at map at <console>:131\n",
       "numTerms = 1000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val lemmatized = lemmatizedWithDate.map { case (docid, date, text) => (docid, text) }\n",
    "    val numTerms = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    val (termDocMatrix, termIds, docIds, idfs) = com.cloudera.datascience.lsa.ParseWikipedia.termDocumentMatrix(lemmatized, stopwordsBroadcast, numTerms, sc);\n",
    "    termDocMatrix.cache();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val termIdsTxT = termIds.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "\n",
    "val outputtermIds= \"data/termIds.txt\"\n",
    "sc.makeRDD(termIdsTxT.toList).saveAsTextFile(\"outputtermIds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mat = new RowMatrix(termDocMatrix)\n",
    "val k = 10 // nombre de valeurs singuliÃ¨res Ã  garder\n",
    "val svd = mat.computeSVD(k, computeU=true)\n",
    "val projections = mat.multiply(svd.V)\n",
    "val projectionsTxt = projections.rows.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "// Delete the existing path, ignore any exceptions thrown if the path doesn't exist\n",
    "val outputProjection = \"data/projection_LSA.txt\"\n",
    "\n",
    "projectionsTxt.saveAsTextFile(outputProjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nbClusters = 10\n",
       "nbIterations = 1000\n",
       "runs = 10\n",
       "clustering = org.apache.spark.mllib.clustering.KMeansModel@669cb1c2\n",
       "classes = MapPartitionsRDD[146] at map at KMeansModel.scala:85\n",
       "outputClasses = data/classes_LSA.txt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data/classes_LSA.txt"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        val nbClusters = 10\n",
    "        val nbIterations = 1000\n",
    "        val runs = 10\n",
    "        val clustering = KMeans.train(termDocMatrix, nbClusters, nbIterations, runs, \"k-means||\", 0)\n",
    "        /*val outputClustering = \"hdfs://head.local:9000/user/emeric/clusters\"\n",
    "        try { hdfs.delete(new org.apache.hadoop.fs.Path(outputClustering), true) } \n",
    "        catch { case _ : Throwable => { } }\n",
    "        clustering.save(sc, outputClustering)*/\n",
    "        \n",
    "        val classes = clustering.predict(termDocMatrix)\n",
    "        val outputClasses = \"data/classes_LSA.txt\"\n",
    "    \n",
    "        classes.saveAsTextFile(outputClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "outputData = MapPartitionsRDD[161] at map at <console>:196\n",
       "outputDataFile = output/output_LSA.txt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "output/output_LSA.txt"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "        val outputData = lemmatizedWithDate.zip(classes).map { case ((docid, date, title),cl) => (docid, date, cl) }.sortBy(_._2).map(l => l.toString.filter(c => c != '(' & c != ')'))\n",
    "        val outputDataFile = \"data/output_LSA.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputDataFile), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        outputData.saveAsSingleTextFile(outputDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "       \n",
    "        clustering.clusterCenters.foreach(clusterCenter => {\n",
    "            val highest = clusterCenter.toArray.zipWithIndex.sortBy(-_._1).map(v => v._2).take(10)\n",
    "            println(\"*****\")\n",
    "            highest.foreach { s => print( termIds(s) + \",\" ) }\n",
    "            println ()\n",
    "            }\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w2vModel = org.apache.spark.mllib.feature.Word2VecModel@78c028b5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'class org.apache.spark.sql.catalyst.QualifiedTableName'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'QualifiedTableName.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.Word2VecModel@78c028b5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.Word2VecModel\n",
    "// lire le Word2VecModel\n",
    "val w2vModel = Word2VecModel.load(sc, \"w2vModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vectors = w2vModel.getVectors.mapValues(vv => org.apache.spark.mllib.linalg.Vectors.dense(vv.map(_.toDouble))).map(identity)\n",
    "        // transmettre la map aux noeuds de calcul\n",
    "val bVectors = sc.broadcast(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:216: error: not found: value bVectors2\n",
       "                          val wvec = bVectors2.value.get(word)\n",
       "                                     ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "        val idTerms = termIds.map(_.swap)\n",
    "\n",
    "        val pairs = termDocMatrix.zip(lemmatized);\n",
    "\n",
    "        var w2vecRepr = pairs.map({ case (row, (docid, lemmas)) => \n",
    "          var vSum = Vectors.zeros(100)\n",
    "          var totalWeight: Double = 0\n",
    "          val words = lemmas.toSet\n",
    "          words.foreach { word =>\n",
    "               val colId = idTerms.get(word);\n",
    "               if (colId != None) {\n",
    "                   var weight = row(colId.get);\n",
    "                   if (! weightTfIdf) {\n",
    "                       weight = 1\n",
    "                   }\n",
    "                   val wvec = bVectors.value.get(word)\n",
    "                   if (wvec != None) {          \n",
    "                       vSum = add(scalarMultiply(weight, wvec.get), vSum)\n",
    "                       totalWeight += weight\n",
    "                   }\n",
    "               }\n",
    "          }\n",
    "          if (totalWeight > 0) {\n",
    "               vSum = scalarMultiply(1.0 / totalWeight, vSum)\n",
    "          }\n",
    "          else {\n",
    "               vSum = Vectors.zeros(100);\n",
    "          }\n",
    "          (docid, vSum)\n",
    "        })/*.filter(vec => Vectors.norm(vec._2, 1.0) > 0.0)*/.persist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:194: error: not found: value w2vecRepr\n",
       "       val matRDD = w2vecRepr.map{v => v._2}.cache()\n",
       "                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matRDD = w2vecRepr.map{v => v._2}.cache()\n",
    "        val mat = new RowMatrix(matRDD)\n",
    "        val matrixTxt = mat.rows.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "        // Delete the existing path, ignore any exceptions thrown if the path doesn't exist\n",
    "        val outputMatrix = \"data/matrice_W2V.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputMatrix), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        matrixTxt.saveAsTextFile(outputMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        val centRed = new StandardScaler(withMean = true, withStd = true).fit(matRDD)\n",
    "        val matCR: RowMatrix = new RowMatrix(centRed.transform(matRDD))\n",
    "\n",
    "        val matCompPrinc = matCR.computePrincipalComponents(10)\n",
    "        val projections = matCR.multiply(matCompPrinc)\n",
    "        //val matSummary = projections.computeColumnSummaryStatistics()\n",
    "        val projectionsTxt = projections.rows.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "        // Delete the existing path, ignore any exceptions thrown if the path doesn't exist\n",
    "        val outputProjection = \"data/projection_W2V.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputProjection), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        projectionsTxt.saveAsTextFile(outputProjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     val nbClusters = 10\n",
    "        val nbIterations = 1000\n",
    "        val runs = 10\n",
    "        val clustering = KMeans.train(matRDD, nbClusters, nbIterations, runs, \"k-means||\", 0)\n",
    "        /*val outputClustering = \"hdfs://head.local:9000/user/emeric/clusters\"\n",
    "        try { hdfs.delete(new org.apache.hadoop.fs.Path(outputClustering), true) } \n",
    "        catch { case _ : Throwable => { } }\n",
    "        clustering.save(sc, outputClustering)*/\n",
    "        \n",
    "        val classes = clustering.predict(matRDD)\n",
    "        val outputClasses = \"data/classes_W2V.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputClasses), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        classes.saveAsTextFile(outputClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        val outputData = lemmatizedWithDate.zip(classes).map { case ((docid, date, title),cl) => (docid, date, cl) }.sortBy(_._2).map(l => l.toString.filter(c => c != '(' & c != ')'))\n",
    "        val outputDataFile = \"data/output_W2V.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputDataFile), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        outputData.saveAsTextFile(outputDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   clustering.clusterCenters.foreach(clusterCenter => {\n",
    "            val nearest = w2vecRepr.map{v => (v._1, Vectors.sqdist(v._2,clusterCenter))}.sortBy(_._2).map{ case (id, dist) => id }.take(10).toSet\n",
    "            println(\"*****\")\n",
    "            pairs.filter{ case (row, (docid, lemmas)) => nearest contains docid }.foreach {  case (row, (docid, lemmas)) => {\n",
    "                val id = row.toArray.zipWithIndex.sortBy(- _._1).take(5).map(_._2)\n",
    "                id.foreach { s => print( termIds(s) + \",\" ) }\n",
    "                println ()\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Word2VecModel\n",
    "// lire le Word2VecModel\n",
    "val w2vModel = Word2VecModel.load(sc, \"w2vModelwiki8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
